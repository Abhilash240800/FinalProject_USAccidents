{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.utils import to_categorical\n",
    "import feather\n",
    "import random\n",
    "from datetime import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"Datasets/final_datasets/merged_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feather.read_dataframe(f\"{FILENAME}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove those one-hot encoded columns in the dataset created by Abhi\n",
    "\n",
    "def in_name(c, one_hots):\n",
    "    for col_name in one_hots:\n",
    "        if c.startswith(f\"{col_name}_\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "one_hots = ['Wind_Direction', 'Timezone','State', 'Weather_Condition']\n",
    "remove = [col for col in data.columns if in_name(col, one_hots)]\n",
    "df = data.drop(remove, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2922400\n"
     ]
    }
   ],
   "source": [
    "# Remove missing values\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Severity'] = df['Severity'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean wind direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"CALM\" = [\"Calm\", \"CALM\"]\n",
    "* \"W\" = [\"W\", \"West\", \"WSW\", \"WNW\"] \n",
    "* \"S\" = [\"S\", \"South\", \"SSW\", \"SSE\"] \n",
    "* \"N\" = [\"N\", \"North\", \"NNW\", \"NNE\"] \n",
    "* \"E\" = [\"E\", \"East\", \"ESE\", \"ENE\"]\n",
    "* \"VAR\" = [\"VAR\", \"Variable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind Direction after simplification:  ['SW' 'S' 'W' 'NW' 'N' 'VAR' 'SE' 'E' 'NE' 'CALM']\n"
     ]
    }
   ],
   "source": [
    "df.loc[df['Wind_Direction']=='Calm','Wind_Direction'] = 'CALM'\n",
    "df.loc[(df['Wind_Direction']=='West')|(df['Wind_Direction']=='WSW')|(df['Wind_Direction']=='WNW'),'Wind_Direction'] = 'W'\n",
    "df.loc[(df['Wind_Direction']=='South')|(df['Wind_Direction']=='SSW')|(df['Wind_Direction']=='SSE'),'Wind_Direction'] = 'S'\n",
    "df.loc[(df['Wind_Direction']=='North')|(df['Wind_Direction']=='NNW')|(df['Wind_Direction']=='NNE'),'Wind_Direction'] = 'N'\n",
    "df.loc[(df['Wind_Direction']=='East')|(df['Wind_Direction']=='ESE')|(df['Wind_Direction']=='ENE'),'Wind_Direction'] = 'E'\n",
    "df.loc[df['Wind_Direction']=='Variable','Wind_Direction'] = 'VAR'\n",
    "print(\"Wind Direction after simplification: \", df['Wind_Direction'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean weather condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(df['Weather_Condition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.timeanddate.com/weather/glossary.html <br>\n",
    "\n",
    "**Create features according the different weather conditions** <br>\n",
    "Scattered clouds, partly cloudy = PARTLY CLOUDY <br>\n",
    "Overcast, Mostly cloudly, Cloudy = CLOUDY <br>\n",
    "Fair, Clear = CLEAR <br>\n",
    "Snow (light snow, heavy snow), and wintry mix  = SNOW <br>\n",
    "drizzle, light rain/drizzle, light freezing rain/drizzle = LIGHT RAIN <br>\n",
    "Heavy Rain, heavy thunderstorms, heavy t-storm = HEAVY RAIN <br>\n",
    "Haze, fog, mist, smoke = fog     *(see https://www.worldatlas.com/articles/what-are-the-differences-between-mist-haze-and-fog.html)*<br>\n",
    "Rain, light thunderstorms, thunderstorms, t-storm, thunder = RAIN <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition(string):\n",
    "    \n",
    "    conditions = ['Clear', 'Partly Cloudy', 'Cloudy', 'Snow', 'Light Rain', 'Heavy Rain', 'Rain', 'Fog'] \n",
    "    strings = [['clear', 'fair'],\n",
    "              ['partly cloudy', 'scattered clouds'],\n",
    "              ['overcast', 'mostly cloudy', 'cloudy'],\n",
    "              ['snow', 'wintry mix'],\n",
    "              ['drizzle', 'light rain', 'light freezing rain'],\n",
    "              ['heavy rain', 'heavy thunderstorms', 'heavy t-storm'],\n",
    "              ['rain', 'thunderstorms', 't-storm', 'thunder', 'showers'],\n",
    "              ['fog', 'haze', 'mist', 'smoke']]\n",
    "\n",
    "    for j, k in enumerate(strings):\n",
    "        for val in k:\n",
    "            if val in string.lower():\n",
    "                return conditions[j]\n",
    "    return 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in df['Weather_Condition'].values:\n",
    "    res.append(get_condition(i))\n",
    "\n",
    "df['Condition'] = res\n",
    "df[['Condition', 'Weather_Condition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conditions = ['Clear', 'Partly Cloudy', 'Cloudy', 'Snow', 'Light Rain', 'Heavy Rain', 'Rain', 'Fog', 'Others'] \n",
    "\n",
    "for i in conditions:\n",
    "    print(i, \":\")\n",
    "    print(set(df[df['Condition']==i]['Weather_Condition']))\n",
    "    print(sum(df['Condition']==i))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "df = df[df['Condition']!='Others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode ['Wind_Direction', 'Timezone', 'State', 'Condition']\n",
    "\n",
    "one_hots = ['Wind_Direction', 'Timezone','State', 'Condition']\n",
    "oh = pd.DataFrame()\n",
    "for c in one_hots:\n",
    "    dummies = pd.get_dummies(df[c], prefix=c)\n",
    "    oh = pd.concat([oh, dummies], axis=1)\n",
    "\n",
    "df = pd.concat([df, oh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this dataframe\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_feather(\"{}_v1.feather\".format(FILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOW CORRELATION BELOW (CHANGE THIS TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With numeric attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "#BUM NOEXIT ROUNABOUT GIVEWAY removed due to badly distributed data\n",
    "df = feather.read_dataframe(\"{}_v1.feather\".format(FILENAME))\n",
    "drop = ['Unnamed: 0', 'ID', 'Start_Lat', 'Start_Lng', 'Start_Time', 'End_Time', 'Weather_Timestamp', 'Description', \n",
    "        'Distance(mi)', \n",
    "        'County', 'City', 'Airport_Code', \n",
    "        'Wind_Direction', 'Timezone', 'State', 'Condition', 'Weather_Condition',\n",
    "        'Street', 'Bump', 'No_Exit', 'Roundabout', 'Give_Way', 'Traffic_Calming',\n",
    "        \"Wind_Direction\"]\n",
    "\n",
    "df = df.drop(drop, axis=1).reset_index(drop=True)\n",
    "\n",
    "# Convert to feather file format\n",
    "df.to_feather(\"{}_numeric_v2.feather\".format(FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without numeric attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "#BUM NOEXIT ROUNABOUT GIVEWAY removed due to badly distributed data\n",
    "df = feather.read_dataframe(\"{}_v1.feather\".format(FILENAME))\n",
    "drop = ['Unnamed: 0', 'ID', 'Start_Lat', 'Start_Lng', 'Start_Time', 'End_Time', 'Weather_Timestamp', 'Description', \n",
    "        'Distance(mi)', \n",
    "        'County', 'City', 'Airport_Code', \n",
    "        'Wind_Direction', 'Timezone', 'State', 'Condition', 'Weather_Condition',\n",
    "        'Street', 'Bump', 'No_Exit', 'Roundabout', 'Give_Way', 'Traffic_Calming',\n",
    "       'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "       'Population_County', 'Drive_County', 'Transit_County', 'Walk_County', 'MedianHouseholdIncome_County']\n",
    "\n",
    "df = df.drop(drop, axis=1).reset_index(drop=True)\n",
    "\n",
    "# Convert to feather file format\n",
    "df.to_feather(\"{}_v2.feather\".format(FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing outliers in numeric data\n",
    "def outlier_removal(data,columns):\n",
    "    for col in columns:\n",
    "        Q1 = np.quantile(data[col].values,0.25)\n",
    "        Q3 = np.quantile(data[col].values,0.75)\n",
    "        IQR = Q3-Q1\n",
    "        \n",
    "        lower = Q1 - 1.5*IQR\n",
    "        upper = Q3+ 1.5*IQR\n",
    "        \n",
    "        data = data[(data[col]>=lower) & (data[col]<=upper)]\n",
    "    return data\n",
    "\n",
    "# Model Function\n",
    "def Neural_Network(data,neuron_size,epoch_size):\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    X = scaler.fit_transform(data)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=3, random_state=30034, shuffle=False)\n",
    "\n",
    "    fold_no = 1\n",
    "    accs = []  # store each accuracy\n",
    "\n",
    "\n",
    "    for train_index, test_index in skf.split(X, labels):\n",
    "\n",
    "        # One hot encode labels\n",
    "        print(\"FOLD NUMBER = \", str(fold_no))\n",
    "\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "\n",
    "        y_train = labels[train_index]\n",
    "        y_test = labels[test_index]\n",
    "\n",
    "        EPOCHS = epoch_size\n",
    "        BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(neuron_size,input_dim=X_train.shape[1], activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(int(2/3*neuron_size), activation=tf.nn.leaky_relu))\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "        (test_loss, test_acc) = model.evaluate(X_test, y_test)\n",
    "        print('Accuracy:', test_acc)\n",
    "        accs.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "        y_preds = model.predict(X_test)\n",
    "        y_preds = np.where(y_preds < 0.5,0,1)\n",
    "\n",
    "\n",
    "        print(\"accuracy:\", accuracy_score(y_test,y_preds))\n",
    "\n",
    "        cm=confusion_matrix(y_test,y_preds)\n",
    "        cm = pd.DataFrame(cm, index = [i for i in \"01\"],\n",
    "                          columns = [i for i in \"01\"])\n",
    "        cm.index.name = 'Actual'\n",
    "        cm.columns.name = 'Predicted'\n",
    "        sns.heatmap(cm, annot=True, \n",
    "                    cmap='Blues', cbar=False)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"============================================\")\n",
    "        fold_no+=1\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation with numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Grouped_Severity'] = np.where(data['Severity']<=2, 0, 1)\n",
    "data = feather.read_dataframe(\"{}_numeric_v2.feather\".format(FILENAME))\n",
    "labels = data['Severity']\n",
    "X = data.drop(['Severity',\"Grouped_Severity\"], axis=1)\n",
    "\n",
    "accuracies = Neural_Network(data,50,10)\n",
    "for i in range(1,4):\n",
    "    print(\"Fold \",i,\" Accuracy: \",accuracies[i])\n",
    "print(\"Average Accuracy: \",np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation without numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feather.read_dataframe(\"{}_v2.feather\".format(FILENAME))\n",
    "labels = data['Severity']\n",
    "X = data.drop(['Severity',\"Grouped_Severity\"], axis=1)\n",
    "\n",
    "accuracies = Neural_Network(data,50,10)\n",
    "for i in range(1,4):\n",
    "    print(\"Fold \",i,\" Accuracy: \",accuracies[i])\n",
    "print(\"Average Accuracy: \",np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best neuron size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feather.read_dataframe(\"{}_numeric_v2.feather\".format(FILENAME))\n",
    "\n",
    "data['Grouped_Severity'] = np.where(data['Severity']<=2, 0, 1)    \n",
    "y = data['Grouped_Severity']\n",
    "X = data.drop(['Severity',\"Grouped_Severity\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=30034)\n",
    "\n",
    "max_neurons = X_train.shape[1]\n",
    "neuron_size = range(max_neurons,4,-10)\n",
    "\n",
    "accs = []\n",
    "\n",
    "for i in neuron_size:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(i,input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(int(2/3*i), activation=tf.nn.leaky_relu))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=128)\n",
    "    (test_loss, test_acc) = model.evaluate(X_test, y_test)\n",
    "    print('Accuracy:', test_acc)\n",
    "    accs.append(test_acc)\n",
    "\n",
    "#plotting line graph of accuracy vs neuron_size\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.plot(neuron_size, accuracies, label=\"Accuracy\")\n",
    "\n",
    "    \n",
    "best_neuron_size = neuron_size[np.argmax(accs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing  outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = feather.read_dataframe(\"{}_numeric_v2.feather\".format(FILENAME))\n",
    "\n",
    "numeric_cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', \n",
    "        'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "       'Population_County', 'Drive_County', 'Transit_County', 'Walk_County', 'MedianHouseholdIncome_County']\n",
    "data = outlier_removal(data,numeric_cols).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot after outlier removal\n",
    "cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', \n",
    "        'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "       'Population_County', 'Drive_County', 'Transit_County', 'Walk_County', 'MedianHouseholdIncome_County']\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "f, axes = plt.subplots(6,2, figsize=(15,20))\n",
    "y = 0;\n",
    "for name in cols[1:]:\n",
    "    i, j = divmod(y, 2)\n",
    "    sns.boxplot(x=data[name], ax=axes[i, j])\n",
    "    y = y+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD NUMBER =  1\n",
      "Epoch 1/20\n",
      "850661/850661 [==============================] - 8s 9us/sample - loss: 0.4992 - acc: 0.7364\n",
      "Epoch 2/20\n",
      "850661/850661 [==============================] - 8s 9us/sample - loss: 0.4837 - acc: 0.7486\n",
      "Epoch 3/20\n",
      "850661/850661 [==============================] - 8s 9us/sample - loss: 0.4784 - acc: 0.7526\n",
      "Epoch 4/20\n",
      "850661/850661 [==============================] - 8s 9us/sample - loss: 0.4754 - acc: 0.7551\n",
      "Epoch 5/20\n",
      "221184/850661 [======>.......................] - ETA: 6s - loss: 0.4722 - acc: 0.7563"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-36ca1a0a204f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;31m# Callbacks batch end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m         \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[0;32m    253\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3493\u001b[0m     \"\"\"\n\u001b[0;32m   3494\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[1;32m-> 3495\u001b[1;33m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[0;32m   3496\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3497\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3401\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3403\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3404\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3526\u001b[0m             \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3527\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3528\u001b[1;33m         \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data['Grouped_Severity'] = np.where(data['Severity']<=2, 0, 1)\n",
    "labels = data['Grouped_Severity']\n",
    "#dropping percipitation and visibility due to all values removed as outliers\n",
    "X = data.drop(['Severity',\"Grouped_Severity\",'Precipitation(in)','Visibility(mi)'], axis=1)\n",
    "\n",
    "accuracies = Neural_Network(data,best_neuron_size,20)\n",
    "for i in range(1,4):\n",
    "    print(\"Fold \",i,\" Accuracy: \",accuracies[i])\n",
    "print(\"Average Accuracy: \",np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
